{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"深度学习建模流程\n",
    "\n",
    "# stage 1 模型选择    : 确定神经网络的层数，每层神经元的个数以及激活函数\n",
    "# stage 2 确定目标函数 : 构建包含模型参数的函数方程，函数取值与建模目标一致，大多情况求解方程的极小值\n",
    "# stage 3 选择优化算法 : 根据损失函数的函数特性，选择最优化算法，以减少算力消耗\n",
    "# stage 4 模型训练    : 利用优化算法求解损失函数得到模型参数（连接神经元的参数取值）\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机模块\n",
    "import random\n",
    "\n",
    "# 绘图模块\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset,DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 导入自定义模块\n",
    "from torchLearning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtensorGenReg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "回归类数据集创建函数\n",
       "\n",
       "param num_examples:创建数据集的数据量\n",
       "param w:包括截距的特征系数向量\n",
       "param bias:是否需要截距\n",
       "param delta:扰动项取值\n",
       "param def:方程次数\n",
       "return :生成的特征张量和标签张量\n",
       "\n",
       "该函数无法创建带有交叉项的方程 eg:deg>=2 y=x1^2 + x1x2(交叉项) + x2^2\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Projects/pytorch_learning/torchLearning.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorGenReg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据集 y = 2x1 - x2 +1\n",
    "torch.manual_seed(420)\n",
    "\n",
    "features,labels = tensorGenReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建模流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X,w):\n",
    "    return torch.mm(X,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定目标函数\n",
    "def squared_loss(y_hat,y):\n",
    "    num_ = y.numel()\n",
    "    sse = torch.sum((y_hat.reshape(-1,1) - y.reshape(-1,1)) ** 2)\n",
    "    \n",
    "    return sse/num_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化算法\n",
    "def sgd(params,lr):\n",
    "    params.data -= lr*params.grad\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -= 之类的操作：in-place operation(对 原对象 修改操作)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于可微张量的 in-place operation 的相关讨论\n",
    "(1) 正常情况下，可微张量的 in-place operation 会导致系统无法区分叶节点 和 其他节点的问题\n",
    "\n",
    "修改后 叶节点 可能不再是 叶节点，计算图是否还会有意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor(2.,requires_grad=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.is_leaf # w是创建的可微张量，是个叶节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开启可微之后，w的所有计算都会被纳入计算图中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = w * 2\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果在计算过程中，使用in-place operation，让新生成的值替换w原始值，则会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-037ca4ab2272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "w = torch.tensor(2.,requires_grad = True)\n",
    "w -= w*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用其他方法对w进行修改，不过w将不再是叶节点，也不能通过反向传播求其导数了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor(2.,requires_grad = True)\n",
    "w = w*2\n",
    "w.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.backward() # w已经成为输出节点，即使w存在在当前操作空间中，但没有任何变量指向它，相当于丢失，之后也没办法使用任何方法对其梯度进行查看，计算图也没了存在意义（核心价值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-d01581d16022>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  w.grad\n"
     ]
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）叶节点数值修改方法\n",
    "\n",
    "   如果出现了一定要修改叶节点取值的情况，典型的如梯度下降过程中利用梯度值修改参数值时，可以使用此前介绍的暂停追踪的方法，如使用with torch.no_grad()语句或者torch.detach_()方法，使得修改叶节点数值时暂停追踪，然后再生成新的叶节点带入计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2., requires_grad=True), True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用 with torch.no_grad()暂停追踪\n",
    "\n",
    "w = torch.tensor(2.,requires_grad = True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    w -= w * 2\n",
    "\n",
    "w,w.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.), True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor(2.,requires_grad = True)\n",
    "\n",
    "w.detach_()\n",
    "w -= w * 2\n",
    "\n",
    "w,w.is_leaf # 注意requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2., requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.requires_grad = True\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2., requires_grad=True), True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用 torch.detach_()暂停追踪\n",
    "\n",
    "w = torch.tensor(2.,requires_grad = True)\n",
    "\n",
    "w.detach_()\n",
    "w -= w * 2\n",
    "w.requires_grad = True\n",
    "\n",
    "w,w.is_leaf # 注意requires_grad可微属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) tensor(2., requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(-2., requires_grad=True), True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用.data来返回可微张量的取值，避免在修改的过程中被追踪\n",
    "w = torch.tensor(2.,requires_grad = True)\n",
    "\n",
    "w.data # .data查看张量的数值，但不改变张量本身的可微性\n",
    "print(w.data,w)\n",
    "\n",
    "w.data -= w * 2 # 对其数值进行修改\n",
    "\n",
    "w,w.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,loss 0.000128\n",
      "epoch 2,loss 0.000101\n",
      "epoch 3,loss 0.000103\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(420)\n",
    "\n",
    "# 初始化核心参数\n",
    "batch_size = 10 # 每一个小批量数\n",
    "lr = 0.03 # 学习率\n",
    "num_epochs = 3 # 训练遍历次数\n",
    "w = torch.zeros(3,1,requires_grad = True) #随机设置初始权重\n",
    "\n",
    "# 参与训练的模型方程\n",
    "net = linreg #使用回归方程 torch.mm(X,w)\n",
    "loss = squared_loss # MSE作为损失函数\n",
    "\n",
    "# 模型训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter(batch_size,features,labels):\n",
    "        l = loss(net(X,w),y)\n",
    "        l.backward()\n",
    "        sgd(w,lr)\n",
    "    train_l = loss(net(features,w),labels)\n",
    "    print('epoch %d,loss %f' % (epoch + 1,train_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,loss 0.000129\n",
      "epoch 2,loss 0.000102\n",
      "epoch 3,loss 0.000102\n"
     ]
    }
   ],
   "source": [
    "# 随机模块\n",
    "import random\n",
    "\n",
    "# 绘图模块\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset,DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 导入自定义模块\n",
    "from torchLearning import *\n",
    "\n",
    "# 生成数据集 y = 2x1 - x2 +1\n",
    "torch.manual_seed(420)\n",
    "\n",
    "features,labels = tensorGenReg()\n",
    "\n",
    "# 建模流程\n",
    "def linreg(X,w):\n",
    "    return torch.mm(X,w)\n",
    "\n",
    "# 确定目标函数\n",
    "def squared_loss(y_hat,y):\n",
    "    num_ = y.numel()\n",
    "    sse = torch.sum((y_hat.reshape(-1,1) - y.reshape(-1,1)) ** 2)\n",
    "    \n",
    "    return sse/num_\n",
    "\n",
    "# 定义优化算法\n",
    "def sgd(params,lr):\n",
    "    params.data -= lr*params.grad\n",
    "    params.grad.zero_()\n",
    "    \n",
    "# 训练模型\n",
    "torch.manual_seed(420)\n",
    "\n",
    "# 初始化核心参数\n",
    "batch_size = 10 # 每一个小批量数\n",
    "lr = 0.03 # 学习率\n",
    "num_epochs = 3 # 训练遍历次数\n",
    "w = torch.zeros(3,1,requires_grad = True) #随机设置初始权重\n",
    "\n",
    "# 参与训练的模型方程\n",
    "net = linreg #使用回归方程 torch.mm(X,w)\n",
    "loss = squared_loss # MSE作为损失函数\n",
    "\n",
    "# 模型训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter(batch_size,features,labels):\n",
    "        l = loss(net(X,w),y)\n",
    "        l.backward()\n",
    "        sgd(w,lr)\n",
    "    train_l = loss(net(features,w),labels)\n",
    "    print('epoch %d,loss %f' % (epoch + 1,train_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0003],\n",
       "        [-1.0002],\n",
       "        [ 1.0008]], requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机模块\n",
    "import random\n",
    "\n",
    "# 绘图模块\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset,DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 导入自定义模块\n",
    "from torchLearning import *\n",
    "\n",
    "# 生成数据集 y = 2x1 - x2 +1\n",
    "torch.manual_seed(420)\n",
    "\n",
    "features,labels = tensorGenReg()\n",
    "\n",
    "# 建模流程\n",
    "def linreg(X,w):\n",
    "    return torch.mm(X,w)\n",
    "\n",
    "# 确定目标函数\n",
    "def squared_loss(y_hat,y):\n",
    "    num_ = y.numel()\n",
    "    sse = torch.sum((y_hat.reshape(-1,1) - y.reshape(-1,1)) ** 2)\n",
    "    \n",
    "    return sse/num_\n",
    "\n",
    "# 定义优化算法\n",
    "def sgd(params,lr):\n",
    "    params.data -= lr*params.grad\n",
    "    params.grad.zero_()\n",
    "    \n",
    "# 训练模型\n",
    "torch.manual_seed(420)\n",
    "writer = SummaryWriter(log_dir='reg_loss')\n",
    "\n",
    "# 初始化核心参数\n",
    "batch_size = 10 # 每一个小批量数\n",
    "lr = 0.03 # 学习率\n",
    "num_epochs = 3 # 训练遍历次数\n",
    "w = torch.zeros(3,1,requires_grad = True) #随机设置初始权重\n",
    "\n",
    "# 参与训练的模型方程\n",
    "net = linreg #使用回归方程 torch.mm(X,w)\n",
    "loss = squared_loss # MSE作为损失函数\n",
    "\n",
    "# 模型训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter(batch_size,features,labels):\n",
    "        l = loss(net(X,w),y)\n",
    "        l.backward()\n",
    "        sgd(w,lr)\n",
    "    train_l = loss(net(features,w),labels)\n",
    "    writer.add_scalar('mul',train_l,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terminal启动服务读取文件\n",
    "# $ tensorboard --logdir=\"reg_loss\"\n",
    "# localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 快速实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义核心参数\n",
    "batch_size = 10 # 每一个小批量数\n",
    "lr = 0.03 # 学习率\n",
    "num_epochs = 3 # 训练遍历次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "torch.manual_seed(420)\n",
    "\n",
    "# 创建数据集\n",
    "features,labels = tensorGenReg()\n",
    "features = features[:, :-1] # 剔除最后全是1的列\n",
    "data = TensorDataset(features,labels) # 数据封装\n",
    "batchData = DataLoader(data,batch_size = batch_size,shuffle = True) # 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0070,  0.5044],\n",
       "        [ 0.6704, -0.3829],\n",
       "        [ 0.0302,  0.3826],\n",
       "        ...,\n",
       "        [-0.9164, -0.6087],\n",
       "        [ 0.7815,  1.2865],\n",
       "        [ 1.4819,  1.1390]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self,in_features=2,out_features=1): # 定义模型的点线结构\n",
    "        super(LR,self).__init__()\n",
    "        self.linear = nn.Linear(in_features,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 实例化模型\n",
    "LR_model = LR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化方法\n",
    "\n",
    "optimizer = optim.SGD(LR_model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "\n",
    "def fit(net,criterion,optimizer,batchdata,epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for X,y in batchdata:\n",
    "            yhat = net.forward(X)\n",
    "            loss = criterion(yhat,y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        writer.add_scalar('loss',loss,global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行模型训练\n",
    "\n",
    "writer = SummaryWriter(log_dir='reg_loss')\n",
    "\n",
    "# 设置随机数种子\n",
    "torch.manual_seed(420)\n",
    "\n",
    "fit(net=LR_model,\n",
    "   criterion = criterion,\n",
    "   optimizer = optimizer,\n",
    "   batchdata = batchData,\n",
    "   epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LR(\n",
       "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模型训练结果\n",
    "\n",
    "LR_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 1.9992, -1.0003]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9994], requires_grad=True)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模型参数\n",
    "\n",
    "list(LR_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算MSE\n",
    "criterion(LR_model(features),labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用 add_graph 方法，在writer中添加 模型的记录图\n",
    "\n",
    "writer.add_graph(LR_model, (features,))\n",
    "# 在graph一栏可以看到拓扑图——模型结构图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单线性回归的局限性\n",
    "\n",
    " 当函数方程次数增加为2及以上的多项式函数关系 或 数据 扰动项增加时，简单线性回归误差将迅速增大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.0552, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deg调大\n",
    "\n",
    "\n",
    "# 定义核心参数\n",
    "batch_size = 10 # 每一个小批量数\n",
    "lr = 0.03 # 学习率\n",
    "num_epochs = 3 # 训练遍历次数\n",
    "\n",
    "# 数据准备\n",
    "torch.manual_seed(420)\n",
    "\n",
    "# 创建数据集\n",
    "features,labels = tensorGenReg(deg=2)\n",
    "features = features[:, :-1] # 剔除最后全是1的列\n",
    "data = TensorDataset(features,labels) # 数据封装\n",
    "batchData = DataLoader(data,batch_size = batch_size,shuffle = True) # 数据加载\n",
    "\n",
    "# 定义模型\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self,in_features=2,out_features=1): # 定义模型的点线结构\n",
    "        super(LR,self).__init__()\n",
    "        self.linear = nn.Linear(in_features,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 实例化模型\n",
    "LR_model = LR()\n",
    "\n",
    "# 定义损失函数\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 定义优化方法\n",
    "\n",
    "optimizer = optim.SGD(LR_model.parameters(), lr=0.03)\n",
    "\n",
    "# 模型训练\n",
    "\n",
    "def fit(net,criterion,optimizer,batchdata,epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for X,y in batchdata:\n",
    "            yhat = net.forward(X)\n",
    "            loss = criterion(yhat,y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        writer.add_scalar('loss',loss,global_step=epoch)\n",
    "        \n",
    "# 执行模型训练\n",
    "\n",
    "writer = SummaryWriter(log_dir='reg_loss')\n",
    "\n",
    "# 设置随机数种子\n",
    "torch.manual_seed(420)\n",
    "\n",
    "fit(net=LR_model,\n",
    "   criterion = criterion,\n",
    "   optimizer = optimizer,\n",
    "   batchdata = batchData,\n",
    "   epochs = num_epochs)\n",
    "\n",
    "# 计算MSE\n",
    "criterion(LR_model(features),labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0959, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delta调大\n",
    "\n",
    "\n",
    "# 定义核心参数\n",
    "batch_size = 10 # 每一个小批量数\n",
    "lr = 0.03 # 学习率\n",
    "num_epochs = 3 # 训练遍历次数\n",
    "\n",
    "# 数据准备\n",
    "torch.manual_seed(420)\n",
    "\n",
    "# 创建数据集\n",
    "features,labels = tensorGenReg(delta=2)\n",
    "features = features[:, :-1] # 剔除最后全是1的列\n",
    "data = TensorDataset(features,labels) # 数据封装\n",
    "batchData = DataLoader(data,batch_size = batch_size,shuffle = True) # 数据加载\n",
    "\n",
    "# 定义模型\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self,in_features=2,out_features=1): # 定义模型的点线结构\n",
    "        super(LR,self).__init__()\n",
    "        self.linear = nn.Linear(in_features,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 实例化模型\n",
    "LR_model = LR()\n",
    "\n",
    "# 定义损失函数\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 定义优化方法\n",
    "\n",
    "optimizer = optim.SGD(LR_model.parameters(), lr=0.03)\n",
    "\n",
    "# 模型训练\n",
    "\n",
    "def fit(net,criterion,optimizer,batchdata,epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for X,y in batchdata:\n",
    "            yhat = net.forward(X)\n",
    "            loss = criterion(yhat,y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        writer.add_scalar('loss',loss,global_step=epoch)\n",
    "        \n",
    "# 执行模型训练\n",
    "\n",
    "writer = SummaryWriter(log_dir='reg_loss')\n",
    "\n",
    "# 设置随机数种子\n",
    "torch.manual_seed(420)\n",
    "\n",
    "fit(net=LR_model,\n",
    "   criterion = criterion,\n",
    "   optimizer = optimizer,\n",
    "   batchdata = batchData,\n",
    "   epochs = num_epochs)\n",
    "\n",
    "# 计算MSE\n",
    "criterion(LR_model(features),labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
