{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN瓶颈\n",
    "1. 网络能够达到的最大深度依然很浅\n",
    "2. 深度网络的训练难度太大（收敛方法有进展：动量法、随机梯度下降）损失更高，精度更低\n",
    "\n",
    "退化：网络深度加深，精度却在下降\n",
    "\n",
    "推测：深层网络中的函数关系本质上就比浅层网络中的函数关系更复杂，更难拟合，因此深层网络本质上就比浅层网络更难优化和训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 残差网络（微软）\n",
    "假设增加深度用的最优结构就是恒等函数y=x，利用恒等函数的性质，将用于加深网络深度的结构项更容易拟合和训练的方向设计，从根源上降低深度网络的训练难度。\n",
    "\n",
    "类似GoogLeNet,在普通的网络结构后串联特殊的结构\n",
    "\n",
    "残差网络使用“残差块（Residual unit）,残差单元”。在残差网络中，将众多残差单元与普通卷积层串联，以实现“在浅层网络后堆叠某种结构、以增加深度的目的”。\n",
    "\n",
    "- y=x => y-x = 0\n",
    "- H(x)-x = F(x) -> 0 ，拟合F(x)即拟合 x 和 0 的关系\n",
    "\n",
    "要得到输出H(x) = 拟合结果F(x) + x,则残差块`并联`：\n",
    "1. 跳跃链接 x\n",
    "2. 卷积链接 F(x)\n",
    "\n",
    "优势：\n",
    "1. 零负担增加深度\n",
    "2. 在实验上表现出比普通卷积网络容易训练的性质，且理论上能保证网络的精度\n",
    "3. 残差单元能大幅增加训练速度\n",
    "\n",
    "残差网络架构：\n",
    "1. 卷积 7*7 stride2\n",
    "2. 最大池化 3*3 stride2\n",
    "3. 几个残差块 3*3 padding=1能保证特征图不变\n",
    "4. 平均池化、线性层、softmax函数\n",
    "\n",
    "- 18/34层：残差单元 Residual Unit\n",
    "- 50/101/152层：瓶颈结构+跳跃链接 Bottlrneck\n",
    "\n",
    "注意：\n",
    "1. 残差单元每个卷积层后有BN层，第一层后有relu函数\n",
    "2. 每个Layer的多个残差块共享一个特征图尺寸\n",
    "3. 每个Layer的第一个残差块的第一个卷积层步长为2\n",
    "4. 为了保证跳跃链接和F(x)的输出的特征图尺寸数量相同以顺利相加，要增加一个1x1卷积核控制\n",
    "5. 每一个瓶颈结构各层输出分别为（middle_out,middle_out,4*middle_out）\n",
    "\n",
    "作为归一化手段，BN层对数据的影响：\n",
    "\n",
    "$$output = \\frac{x-E[x]}{\\sqrt{Varx+\\varepsilon}}*\\gamma+\\beta$$\n",
    "- gamma和beta作为需要学习的参数，若都设置为0，则任何经过BN层的数据都会为0。\n",
    "- nn.BatchNorm2d中beta默认为0，则设置gamma就可以了\n",
    "- 将残差块（残差单元/瓶颈架构）中最后一个卷积层后的BN层上的gamma设置为0，就可以让F(x)的输出结果为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basicconv +BN +relu (conv3x3,conv1x1)\n",
    "# Residual Unit,Bottlrneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Type,Union,List,Optional\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_,out_,stride=1,initialzero = False):\n",
    "    bn = nn.BatchNorm2d(out_)\n",
    "    # 需要进行判断：要对BN进行0初始化吗？\n",
    "    # 最后一层就初始化，不是最后一层就不改变gamma和beta\n",
    "    if initialzero == True:\n",
    "        nn.init.constant_(bn.weight,0)\n",
    "    return nn.Sequential(nn.Conv2d(in_,out_\n",
    "                            ,kernel_size=3,padding=1,stride= stride\n",
    "                            ,bias=False)\n",
    "                         ,bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3x3(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_,out_,stride=1,initialzero = False):\n",
    "    bn = nn.BatchNorm2d(out_)\n",
    "    # 需要进行判断：要对BN进行0初始化吗？\n",
    "    # 最后一层就初始化，不是最后一层就不改变gamma和beta\n",
    "    if initialzero == True:\n",
    "        nn.init.constant_(bn.weight,0)\n",
    "    return nn.Sequential(nn.Conv2d(in_,out_\n",
    "                            ,kernel_size=1,padding=0,stride= stride\n",
    "                            ,bias=False)\n",
    "                         ,bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1x1(2,10,1,True)[1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(nn.Module):\n",
    "    # 这是残差单元类\n",
    "    # stride1是否等于2呢？如果等于2 - 特征图尺寸会发生变化\n",
    "    # 需要在跳跃链接上增加1x1卷积层来调整特征图尺寸\n",
    "    # 如果stride1等于1，则什么也不需要做\n",
    "    def __init__(self,out_:int\n",
    "                 ,stride1: int = 1\n",
    "                 ,in_:Optional[int] = None\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stride1 = stride1\n",
    "        \n",
    "        # 当特征图尺寸需要缩小时，卷积层的输出特征图数量out_等于输入特征图数量in_的2倍\n",
    "        # 当特征图尺寸不需要缩小时，out_ == in_\n",
    "        if stride1 != 1:\n",
    "            in_ = int(out_ / 2)\n",
    "        else:\n",
    "            in_ = out_\n",
    "        \n",
    "        # 拟合部分，输出F(x)\n",
    "        self.fit_ = nn.Sequential(conv3x3(in_,out_,stride=stride1)\n",
    "                                 ,nn.ReLU(inplace=True)\n",
    "                                 ,conv3x3(out_,out_,initialzero=True)\n",
    "                                 )\n",
    "        # 跳跃链接，输出 x (1x1卷积核之后的 x)\n",
    "        self.skipconv = conv1x1(in_,out_,stride=stride1)\n",
    "        \n",
    "        # 单独定义放在H(x)之后来使用的激活函数ReLU\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        fx = self.fit_(x) # 拟合结果\n",
    "        if self.stride1 != 1:\n",
    "            x = self.skipconv(x) # 跳跃链接\n",
    "#         x = x # 跳跃链接\n",
    "        hx = self.relu(fx + x)\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(10,64,56,56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0号残差单元 - 需要特征图折半，特征图数量加倍\n",
    "conv3_x_18_0 = ResidualUnit(out_ = 128,stride1 = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3_x_18_0(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_x_18_0 = ResidualUnit(out_ = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64, 56, 56])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2_x_18_0(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self,middle_out\n",
    "                 ,stride1:int = 1\n",
    "                 ,in_:Optional[int]=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        out_ = 4 * middle_out\n",
    "        \n",
    "        if in_ == None:\n",
    "            # 是需要将特征图尺寸缩小的场合吗？\n",
    "            # conv2_x - conv3_x - conv4_x - conv5_x 相互链接的时候\n",
    "            # 每次都需要将特征图尺寸折半，同时卷积层上的middle_out = 1/2 in_\n",
    "            if stride1 != 1: # 缩小特征图的场合，即这个瓶颈结构是每个layers的第一个瓶颈结构\n",
    "                in_ = middle_out * 2\n",
    "            else: # 不缩小特征图的场合，即这个瓶颈结构是后面的重复结构\n",
    "                in_ = middle_out * 4\n",
    "        else:\n",
    "            in_ = 64\n",
    "        \n",
    "        self.fit_ = nn.Sequential(conv1x1(in_,middle_out,stride=stride1)\n",
    "                                 ,nn.ReLU(inplace=True)\n",
    "                                 ,conv3x3(middle_out,middle_out)\n",
    "                                 ,nn.ReLU(inplace=True)\n",
    "                                 ,conv1x1(middle_out,out_,initialzero=True))\n",
    "        \n",
    "        self.skipconv = conv1x1(in_,out_,stride=stride1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        fx = self.fit_(x)\n",
    "        \n",
    "        # 跳跃链接\n",
    "        x = self.skipconv(x)\n",
    "        hx = self.relu(fx + x)\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 56, 56])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "data1 = torch.ones(10,64,56,56) # conv2x的输入\n",
    "conv2_x_101_0 = Bottleneck(in_=64,middle_out=64)\n",
    "conv2_x_101_0(data1).shape\n",
    "# 特征图尺寸不变，输出尺寸翻四倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 28, 28])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不是conv1后紧跟的第一个瓶颈结构，但是需要缩小特征图尺寸\n",
    "data2 = torch.ones(10,256,56,56)\n",
    "conv3_x_101_0 = Bottleneck(middle_out=128,stride1=2)\n",
    "conv3_x_101_0(data2).shape # 输出翻两倍，特征图尺寸缩小一半"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 28, 28])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不是conv1后紧跟的第一个瓶颈结构，且不需要缩小特征图尺寸\n",
    "data3 = torch.ones(10,512,28,28)\n",
    "conv3_x_101_1 = Bottleneck(128)\n",
    "conv3_x_101_1(data3).shape # 输出数量不变，特征图尺寸也不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks_conv4_x = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv4_x_50 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在列表中添加第0个瓶颈架构块\n",
    "conv4_x_50.append(Bottleneck(middle_out = 256,stride1 = 2))\n",
    "\n",
    "for i in range(num_blocks_conv4_x - 1):\n",
    "    conv4_x_50.append(Bottleneck(middle_out=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conv4_x_50) # 包含了6个块，第一个块是包含步长为2的卷积层，剩下的块是重复结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50-conv2\n",
    "bt0 = Bottleneck(middle_out=64,in_=64)\n",
    "bt1 = Bottleneck(middle_out=64)\n",
    "bt2 = Bottleneck(middle_out=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 除了第一个块，其他的用循环\n",
    "layers = []\n",
    "num_blocks = 6\n",
    "afterconv1 = True # 是conv1之后的第一个块\n",
    "\n",
    "if afterconv1 == True:\n",
    "    layers.append(Bottleneck(middle_out=64,in_=64))\n",
    "else:\n",
    "    layers.append(Bottleneck(middle_out=128,stride1=2))\n",
    "    \n",
    "for i in range(num_blocks-1):\n",
    "    layers.append(Bottleneck(middle_out=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 除了第一个块，其他的用循环\n",
    "layers = []\n",
    "num_blocks = 6\n",
    "afterconv1 = True # 是conv1之后的第一个块\n",
    "\n",
    "if afterconv1 == True:\n",
    "    layers.append(ResidualUnit(out_=64,in_=64))\n",
    "else:\n",
    "    layers.append(ResidualUnit(out_=128,stride1=2))\n",
    "    \n",
    "for i in range(num_blocks-1):\n",
    "    layers.append(ResidualUnit(out_=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type只能填入类Union包括多个类[lei1,...]\n",
    "def make_layers(block: Type[Union[ResidualUnit,Bottleneck]]\n",
    "                ,the_out: int\n",
    "                ,num_blocks: int\n",
    "                ,afterconv1: bool=False):\n",
    "    layers = []\n",
    "    \n",
    "    if afterconv1 == True:\n",
    "        layers.append(block(the_out,in_=64))\n",
    "    else:\n",
    "        layers.append(block(the_out,stride1=2))\n",
    "\n",
    "    for i in range(num_blocks-1):\n",
    "        layers.append(block(the_out))\n",
    "        \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_34_conv4_x = make_layers(ResidualUnit,\n",
    "                              256,\n",
    "                              6,\n",
    "                              False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResidualUnit(\n",
       "   (fit_): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (1): ReLU(inplace=True)\n",
       "     (2): Sequential(\n",
       "       (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (skipconv): Sequential(\n",
       "     (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "     (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (relu): ReLU(inplace=True)\n",
       " ),\n",
       " ResidualUnit(\n",
       "   (fit_): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (1): ReLU(inplace=True)\n",
       "     (2): Sequential(\n",
       "       (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (skipconv): Sequential(\n",
       "     (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (relu): ReLU(inplace=True)\n",
       " ),\n",
       " ResidualUnit(\n",
       "   (fit_): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (1): ReLU(inplace=True)\n",
       "     (2): Sequential(\n",
       "       (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (skipconv): Sequential(\n",
       "     (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (relu): ReLU(inplace=True)\n",
       " ),\n",
       " ResidualUnit(\n",
       "   (fit_): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (1): ReLU(inplace=True)\n",
       "     (2): Sequential(\n",
       "       (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (skipconv): Sequential(\n",
       "     (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (relu): ReLU(inplace=True)\n",
       " ),\n",
       " ResidualUnit(\n",
       "   (fit_): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (1): ReLU(inplace=True)\n",
       "     (2): Sequential(\n",
       "       (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (skipconv): Sequential(\n",
       "     (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (relu): ReLU(inplace=True)\n",
       " ),\n",
       " ResidualUnit(\n",
       "   (fit_): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (1): ReLU(inplace=True)\n",
       "     (2): Sequential(\n",
       "       (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (skipconv): Sequential(\n",
       "     (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (relu): ReLU(inplace=True)\n",
       " )]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_34_conv4_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (1): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (2): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (3): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (4): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (5): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(*layer_34_conv4_x) # 相当于...[],pythonzhongde 星号解析列表/储存器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type只能填入类Union包括多个类[lei1,...]\n",
    "def make_layers(block: Type[Union[ResidualUnit,Bottleneck]]\n",
    "                ,the_out: int\n",
    "                ,num_blocks: int\n",
    "                ,afterconv1: bool=False):\n",
    "    layers = []\n",
    "    \n",
    "    if afterconv1 == True:\n",
    "        layers.append(block(the_out,in_=64))\n",
    "    else:\n",
    "        layers.append(block(the_out,stride1=2))\n",
    "\n",
    "    for i in range(num_blocks-1):\n",
    "        layers.append(block(the_out))\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "# 34层网络，conv2_x,紧跟在conv1后的首个架构\n",
    "# 不缩小特征图尺寸，每层的输出都是64，3个块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               --                        --\n",
       "├─ResidualUnit: 1-1                      [10, 64, 56, 56]          78,208\n",
       "├─ResidualUnit: 1-2                      [10, 64, 56, 56]          78,208\n",
       "├─ResidualUnit: 1-3                      [10, 64, 56, 56]          78,208\n",
       "==========================================================================================\n",
       "Total params: 221,952\n",
       "Trainable params: 221,952\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.94\n",
       "==========================================================================================\n",
       "Input size (MB): 8.03\n",
       "Forward/backward pass size (MB): 192.68\n",
       "Params size (MB): 0.89\n",
       "Estimated Total Size (MB): 201.59\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datashape = (10,64,56,56)\n",
    "conv2_x_34 = make_layers(ResidualUnit,\n",
    "                              64,\n",
    "                              3,\n",
    "                              afterconv1=True)\n",
    "summary(conv2_x_34,datashape,depth=1,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2_x_101 = make_layers(Bottleneck,\n",
    "            64,\n",
    "            3,\n",
    "            afterconv1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               --                        --\n",
       "├─Bottleneck: 1-1                        [10, 256, 56, 56]         --\n",
       "│    └─Sequential: 2-1                   [10, 256, 56, 56]         --\n",
       "│    │    └─Sequential: 3-1              [10, 64, 56, 56]          4,224\n",
       "│    │    └─ReLU: 3-2                    [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-3              [10, 64, 56, 56]          36,992\n",
       "│    │    └─ReLU: 3-4                    [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-5              [10, 256, 56, 56]         16,896\n",
       "│    └─Sequential: 2-2                   [10, 256, 56, 56]         --\n",
       "│    │    └─Conv2d: 3-6                  [10, 256, 56, 56]         16,384\n",
       "│    │    └─BatchNorm2d: 3-7             [10, 256, 56, 56]         512\n",
       "│    └─ReLU: 2-3                         [10, 256, 56, 56]         --\n",
       "├─Bottleneck: 1-2                        [10, 256, 56, 56]         --\n",
       "│    └─Sequential: 2-4                   [10, 256, 56, 56]         --\n",
       "│    │    └─Sequential: 3-8              [10, 64, 56, 56]          16,512\n",
       "│    │    └─ReLU: 3-9                    [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-10             [10, 64, 56, 56]          36,992\n",
       "│    │    └─ReLU: 3-11                   [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-12             [10, 256, 56, 56]         16,896\n",
       "│    └─Sequential: 2-5                   [10, 256, 56, 56]         --\n",
       "│    │    └─Conv2d: 3-13                 [10, 256, 56, 56]         65,536\n",
       "│    │    └─BatchNorm2d: 3-14            [10, 256, 56, 56]         512\n",
       "│    └─ReLU: 2-6                         [10, 256, 56, 56]         --\n",
       "├─Bottleneck: 1-3                        [10, 256, 56, 56]         --\n",
       "│    └─Sequential: 2-7                   [10, 256, 56, 56]         --\n",
       "│    │    └─Sequential: 3-15             [10, 64, 56, 56]          16,512\n",
       "│    │    └─ReLU: 3-16                   [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-17             [10, 64, 56, 56]          36,992\n",
       "│    │    └─ReLU: 3-18                   [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-19             [10, 256, 56, 56]         16,896\n",
       "│    └─Sequential: 2-8                   [10, 256, 56, 56]         --\n",
       "│    │    └─Conv2d: 3-20                 [10, 256, 56, 56]         65,536\n",
       "│    │    └─BatchNorm2d: 3-21            [10, 256, 56, 56]         512\n",
       "│    └─ReLU: 2-9                         [10, 256, 56, 56]         --\n",
       "==========================================================================================\n",
       "Total params: 347,904\n",
       "Trainable params: 347,904\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 10.79\n",
       "==========================================================================================\n",
       "Input size (MB): 8.03\n",
       "Forward/backward pass size (MB): 963.38\n",
       "Params size (MB): 1.39\n",
       "Estimated Total Size (MB): 972.80\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(conv2_x_101,datashape,depth=3,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv4_x_101 = make_layers(Bottleneck,\n",
    "            256,\n",
    "            23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashape = (10,512,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               --                        --\n",
       "├─Bottleneck: 1-1                        [10, 1024, 14, 14]        1,512,448\n",
       "├─Bottleneck: 1-2                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-3                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-4                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-5                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-6                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-7                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-8                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-9                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-10                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-11                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-12                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-13                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-14                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-15                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-16                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-17                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-18                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-19                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-20                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-21                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-22                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-23                       [10, 1024, 14, 14]        2,167,808\n",
       "==========================================================================================\n",
       "Total params: 49,204,224\n",
       "Trainable params: 49,204,224\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 96.21\n",
       "==========================================================================================\n",
       "Input size (MB): 16.06\n",
       "Forward/backward pass size (MB): 1846.48\n",
       "Params size (MB): 196.82\n",
       "Estimated Total Size (MB): 2059.35\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(conv4_x_101,datashape,depth=1,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block: Type[Union[ResidualUnit,Bottleneck]]\n",
    "                ,layers:List[int]\n",
    "                ,num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        block:要使用的用来加深的基本架构是？（残差单元/瓶颈结构）\n",
    "        layers:列表，每个层里具体有多少个块\n",
    "        num_classes:真实标签含有多少个类别\n",
    "        \"\"\"\n",
    "        \n",
    "        # layer1:卷积+池化的组合\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,64\n",
    "                                              ,kernel_size=7,stride=2\n",
    "                                              ,padding=3,bias=False)\n",
    "                                   ,nn.BatchNorm2d(64)\n",
    "                                   ,nn.ReLU(inplace=True)\n",
    "                                   ,nn.MaxPool2d(kernel_size=3\n",
    "                                                 ,stride=2\n",
    "                                                 ,ceil_mode=True))\n",
    "        \n",
    "        # layer2 - layer5:残差块/瓶颈结构\n",
    "        self.layer2_x = make_layers(block,64,layers[0],afterconv1=True)\n",
    "        self.layer3_x = make_layers(block,128,layers[1])        \n",
    "        self.layer4_x = make_layers(block,256,layers[2])        \n",
    "        self.layer5_x = make_layers(block,512,layers[3])        \n",
    "        \n",
    "        # 全局平均池化\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        # 分类\n",
    "        if block == ResidualUnit:\n",
    "            self.fc = nn.Linear(512,num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(2048,num_classes)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x) # layer1. 普通卷积+池化的输出\n",
    "        x = self.layer5_x(self.layer4_x(self.layer3_x(self.layer2_x(x))))\n",
    "        x = self.avgpool(x) # 特征图尺寸1x1 (n_samples,fc,1,1)\n",
    "        x = torch.flatten(x,1) # 将x拉平到1维\n",
    "        x = self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashape = (10,3,224,224) # ImageNet数据集的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "res34 = ResNet(ResidualUnit,[3,4,6,3],num_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "res101 = ResNet(Bottleneck,[3,4,23,3],num_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ResNet                                        --                        --\n",
       "├─Sequential: 1-1                             [10, 64, 56, 56]          --\n",
       "│    └─Conv2d: 2-1                            [10, 64, 112, 112]        9,408\n",
       "│    └─BatchNorm2d: 2-2                       [10, 64, 112, 112]        128\n",
       "│    └─ReLU: 2-3                              [10, 64, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-4                         [10, 64, 56, 56]          --\n",
       "├─Sequential: 1-2                             [10, 64, 56, 56]          --\n",
       "│    └─ResidualUnit: 2-5                      [10, 64, 56, 56]          78,208\n",
       "│    └─ResidualUnit: 2-6                      [10, 64, 56, 56]          78,208\n",
       "│    └─ResidualUnit: 2-7                      [10, 64, 56, 56]          78,208\n",
       "├─Sequential: 1-3                             [10, 128, 28, 28]         --\n",
       "│    └─ResidualUnit: 2-8                      [10, 128, 28, 28]         230,144\n",
       "│    └─ResidualUnit: 2-9                      [10, 128, 28, 28]         312,064\n",
       "│    └─ResidualUnit: 2-10                     [10, 128, 28, 28]         312,064\n",
       "│    └─ResidualUnit: 2-11                     [10, 128, 28, 28]         312,064\n",
       "├─Sequential: 1-4                             [10, 256, 14, 14]         --\n",
       "│    └─ResidualUnit: 2-12                     [10, 256, 14, 14]         919,040\n",
       "│    └─ResidualUnit: 2-13                     [10, 256, 14, 14]         1,246,720\n",
       "│    └─ResidualUnit: 2-14                     [10, 256, 14, 14]         1,246,720\n",
       "│    └─ResidualUnit: 2-15                     [10, 256, 14, 14]         1,246,720\n",
       "│    └─ResidualUnit: 2-16                     [10, 256, 14, 14]         1,246,720\n",
       "│    └─ResidualUnit: 2-17                     [10, 256, 14, 14]         1,246,720\n",
       "├─Sequential: 1-5                             [10, 512, 7, 7]           --\n",
       "│    └─ResidualUnit: 2-18                     [10, 512, 7, 7]           3,673,088\n",
       "│    └─ResidualUnit: 2-19                     [10, 512, 7, 7]           4,983,808\n",
       "│    └─ResidualUnit: 2-20                     [10, 512, 7, 7]           4,983,808\n",
       "├─AdaptiveAvgPool2d: 1-6                      [10, 512, 1, 1]           --\n",
       "├─Linear: 1-7                                 [10, 1000]                513,000\n",
       "===============================================================================================\n",
       "Total params: 21,797,672\n",
       "Trainable params: 21,797,672\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 36.64\n",
       "===============================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 598.18\n",
       "Params size (MB): 87.19\n",
       "Estimated Total Size (MB): 691.39\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(res34,datashape,depth=2,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ResNet                                        --                        --\n",
       "├─Sequential: 1-1                             [10, 64, 56, 56]          --\n",
       "│    └─Conv2d: 2-1                            [10, 64, 112, 112]        9,408\n",
       "│    └─BatchNorm2d: 2-2                       [10, 64, 112, 112]        128\n",
       "│    └─ReLU: 2-3                              [10, 64, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-4                         [10, 64, 56, 56]          --\n",
       "├─Sequential: 1-2                             [10, 256, 56, 56]         --\n",
       "│    └─Bottleneck: 2-5                        [10, 256, 56, 56]         75,008\n",
       "│    └─Bottleneck: 2-6                        [10, 256, 56, 56]         136,448\n",
       "│    └─Bottleneck: 2-7                        [10, 256, 56, 56]         136,448\n",
       "├─Sequential: 1-3                             [10, 512, 28, 28]         --\n",
       "│    └─Bottleneck: 2-8                        [10, 512, 28, 28]         379,392\n",
       "│    └─Bottleneck: 2-9                        [10, 512, 28, 28]         543,232\n",
       "│    └─Bottleneck: 2-10                       [10, 512, 28, 28]         543,232\n",
       "│    └─Bottleneck: 2-11                       [10, 512, 28, 28]         543,232\n",
       "├─Sequential: 1-4                             [10, 1024, 14, 14]        --\n",
       "│    └─Bottleneck: 2-12                       [10, 1024, 14, 14]        1,512,448\n",
       "│    └─Bottleneck: 2-13                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-14                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-15                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-16                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-17                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-18                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-19                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-20                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-21                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-22                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-23                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-24                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-25                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-26                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-27                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-28                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-29                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-30                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-31                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-32                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-33                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-34                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Sequential: 1-5                             [10, 2048, 7, 7]          --\n",
       "│    └─Bottleneck: 2-35                       [10, 2048, 7, 7]          6,039,552\n",
       "│    └─Bottleneck: 2-36                       [10, 2048, 7, 7]          8,660,992\n",
       "│    └─Bottleneck: 2-37                       [10, 2048, 7, 7]          8,660,992\n",
       "├─AdaptiveAvgPool2d: 1-6                      [10, 2048, 1, 1]          --\n",
       "├─Linear: 1-7                                 [10, 1000]                2,049,000\n",
       "===============================================================================================\n",
       "Total params: 76,981,288\n",
       "Trainable params: 76,981,288\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 135.30\n",
       "===============================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 3701.06\n",
       "Params size (MB): 307.93\n",
       "Estimated Total Size (MB): 4015.01\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(res101,datashape,depth=2,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
