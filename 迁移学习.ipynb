{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 迁移学习\n",
    "大型神经网络的训练困难\n",
    "训练大型模型时，用于降低数据需求以及加快训练速度的关键技术——预训练pre-train （迁移学习transfer learning）\n",
    "\n",
    "借用已经训练好的模型来构筑新架构\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于经典架构构建自己的架构时：\n",
    "    - 将经典架构本身复制，在前后增加我们希望的层\n",
    "    - 这个过程中的经典架构并没有被训练过，全部层在训练时都得初始化自己的参数，从0开始训练\n",
    "- 迁移学习\n",
    "    - 复用已经训练好的架构，包括架构本身以及每层上的权重\n",
    "    - 沿用现存架构以及其权重后，在后面加入自定义的层，以此来构筑新的架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐藏层-信息储存在权重中\n",
    "- 网络浅层：提取浅层信息（常识）\n",
    "- 网络深层：提取神经信息（具体信息）\n",
    "\n",
    "可以将浅层信息迁移过去，迁移对象\n",
    "- `任务相似`\n",
    "- 数据高度相似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练时的两种选择：\n",
    "1. 将迁移层上的权重作为初始化工具\n",
    "\n",
    "    将迁移层的权重作为新架构的初始化权重，在此基础上对所有层进行训练，给模型指引方向。严谨文献中称为“预训练”\n",
    "2. 将迁移层作为固定的特征提取工具\n",
    "\n",
    "    将迁移层的权重“固定”起来，不让这些权重收到反向传播等过程的影响，而让他们作为架构中的“固定知识”被一直使用。相对的，自定义添加的层像普通的网络架构一样初始化参数并进行训练，并在每一次迭代中逐渐找到适合自己的权重。严谨文献中称为“迁移学习”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以在刚开始训练时固定所有迁移层的权重，大步长迭代学习下面的隐藏层，等到模型结果逐渐不再持续上升时，尝试解锁一两个层并使用小的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models as m\n",
    "\n",
    "resnet18_ = m.resnet18()\n",
    "\n",
    "# 执行该代码时要关闭VPN\n",
    "# 下载预训练好的权重\n",
    "rs18pt = m.resnet18(pretrained=True) # resnet18_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0149, -0.0052, -0.0291,  0.0267, -0.0076, -0.0162, -0.0188],\n",
       "         [ 0.0222, -0.0382,  0.0143, -0.0383, -0.0120, -0.0095,  0.0253],\n",
       "         [ 0.0099, -0.0146, -0.0072,  0.0347,  0.0028, -0.0095, -0.0026],\n",
       "         [ 0.0011, -0.0047, -0.0086,  0.0197,  0.0021, -0.0307,  0.0407],\n",
       "         [-0.0123, -0.0054,  0.0066,  0.0367,  0.0212, -0.0253,  0.0039],\n",
       "         [ 0.0588, -0.0258, -0.0818, -0.0036,  0.0236, -0.0223,  0.0247],\n",
       "         [-0.0053, -0.0195, -0.0177,  0.0176, -0.0011,  0.0129, -0.0116]],\n",
       "\n",
       "        [[-0.0105,  0.0224, -0.0290, -0.0490, -0.0088, -0.0114, -0.0399],\n",
       "         [-0.0044,  0.0405,  0.0116, -0.0110,  0.0330,  0.0023,  0.0056],\n",
       "         [ 0.0533,  0.0197, -0.0495,  0.0005,  0.0282, -0.0240, -0.0001],\n",
       "         [-0.0111, -0.0017,  0.0689, -0.0112,  0.0120, -0.0182, -0.0368],\n",
       "         [-0.0017, -0.0033,  0.0054,  0.0020,  0.0035, -0.0249, -0.0172],\n",
       "         [-0.0376,  0.0592,  0.0334,  0.0122, -0.0030, -0.0047, -0.0184],\n",
       "         [ 0.0032,  0.0082, -0.0124, -0.0184, -0.0402,  0.0119, -0.0072]],\n",
       "\n",
       "        [[ 0.0219, -0.0418,  0.0345, -0.0478,  0.0184,  0.0331,  0.0124],\n",
       "         [ 0.0153,  0.0514, -0.0051, -0.0083,  0.0072, -0.0083, -0.0298],\n",
       "         [-0.0304, -0.0635,  0.0267,  0.0260,  0.0265,  0.0038, -0.0618],\n",
       "         [ 0.0408,  0.0073,  0.0072,  0.0144, -0.0010, -0.0122,  0.0114],\n",
       "         [ 0.0006, -0.0076, -0.0050, -0.0157,  0.0358,  0.0105, -0.0253],\n",
       "         [-0.0192, -0.0110, -0.0198,  0.0307, -0.0083,  0.0074,  0.0118],\n",
       "         [-0.0158,  0.0008, -0.0313, -0.0042, -0.0236, -0.0352,  0.0001]]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18_.conv1.weight[0] # 初始化的参数，准备好预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0419e-02, -6.1356e-03, -1.8098e-03,  7.4841e-02,  5.6615e-02,\n",
       "           1.7083e-02, -1.2694e-02],\n",
       "         [ 1.1083e-02,  9.5276e-03, -1.0993e-01, -2.8050e-01, -2.7124e-01,\n",
       "          -1.2907e-01,  3.7424e-03],\n",
       "         [-6.9434e-03,  5.9089e-02,  2.9548e-01,  5.8720e-01,  5.1972e-01,\n",
       "           2.5632e-01,  6.3573e-02],\n",
       "         [ 3.0505e-02, -6.7018e-02, -2.9841e-01, -4.3868e-01, -2.7085e-01,\n",
       "          -6.1282e-04,  5.7602e-02],\n",
       "         [-2.7535e-02,  1.6045e-02,  7.2595e-02, -5.4102e-02, -3.3285e-01,\n",
       "          -4.2058e-01, -2.5781e-01],\n",
       "         [ 3.0613e-02,  4.0960e-02,  6.2850e-02,  2.3897e-01,  4.1384e-01,\n",
       "           3.9359e-01,  1.6606e-01],\n",
       "         [-1.3736e-02, -3.6746e-03, -2.4084e-02, -6.5877e-02, -1.5070e-01,\n",
       "          -8.2230e-02, -5.7828e-03]],\n",
       "\n",
       "        [[-1.1397e-02, -2.6619e-02, -3.4641e-02,  3.6812e-02,  3.2521e-02,\n",
       "           6.6221e-04, -2.5743e-02],\n",
       "         [ 4.5687e-02,  3.3603e-02, -1.0453e-01, -3.0885e-01, -3.1253e-01,\n",
       "          -1.6051e-01, -1.2826e-03],\n",
       "         [-8.3730e-04,  9.8420e-02,  4.0210e-01,  7.7035e-01,  7.0789e-01,\n",
       "           3.6887e-01,  1.2455e-01],\n",
       "         [-5.8427e-03, -1.2862e-01, -4.2071e-01, -5.9270e-01, -3.8285e-01,\n",
       "          -4.2407e-02,  6.1568e-02],\n",
       "         [-5.5926e-02, -5.2239e-03,  2.7081e-02, -1.5159e-01, -4.6178e-01,\n",
       "          -5.7080e-01, -3.6552e-01],\n",
       "         [ 3.2860e-02,  5.5574e-02,  9.9670e-02,  3.1815e-01,  5.4636e-01,\n",
       "           4.8276e-01,  1.9867e-01],\n",
       "         [ 5.3051e-03,  6.6938e-03, -1.7254e-02, -6.9806e-02, -1.4822e-01,\n",
       "          -7.7248e-02,  7.2183e-04]],\n",
       "\n",
       "        [[-2.0315e-03, -9.1617e-03,  2.1209e-02,  8.9755e-02,  8.9177e-02,\n",
       "           3.3655e-02, -2.0102e-02],\n",
       "         [ 1.5398e-02, -1.8648e-02, -1.2591e-01, -2.9553e-01, -2.5342e-01,\n",
       "          -1.2980e-01, -2.7975e-02],\n",
       "         [ 9.8454e-03,  4.9047e-02,  2.1699e-01,  4.3010e-01,  3.4872e-01,\n",
       "           1.0433e-01,  1.8413e-02],\n",
       "         [ 2.6426e-02, -2.5990e-02, -1.9699e-01, -2.6806e-01, -1.0524e-01,\n",
       "           7.8577e-02,  1.2077e-01],\n",
       "         [-2.8356e-02,  1.8404e-02,  9.8647e-02,  6.1242e-02, -1.1740e-01,\n",
       "          -2.5760e-01, -1.5451e-01],\n",
       "         [ 2.0766e-02, -2.6286e-03, -3.7825e-02,  5.7450e-02,  2.4141e-01,\n",
       "           2.4345e-01,  1.1796e-01],\n",
       "         [ 7.4684e-04,  7.7677e-04, -1.0050e-02, -5.5153e-02, -1.4865e-01,\n",
       "          -1.1754e-01, -3.8350e-02]]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs18pt.conv1.weight[0] # 经过预训练的参数\n",
    "# grad_fn=<SelectBackward> 可以被反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 属性 requires_grad 为 True，意味着可以参与反向传播\n",
    "# 预训练的参数刚被导入时，都是默认可以被训练的\n",
    "rs18pt.conv1.weight[0].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f8a04368660>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs18pt.parameters() # 类似DataLoader，导入后是生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将导入的预训练模型中所有的参数锁住\n",
    "for param in rs18pt.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0104, -0.0061, -0.0018,  0.0748,  0.0566,  0.0171, -0.0127],\n",
       "        [ 0.0111,  0.0095, -0.1099, -0.2805, -0.2712, -0.1291,  0.0037],\n",
       "        [-0.0069,  0.0591,  0.2955,  0.5872,  0.5197,  0.2563,  0.0636],\n",
       "        [ 0.0305, -0.0670, -0.2984, -0.4387, -0.2709, -0.0006,  0.0576],\n",
       "        [-0.0275,  0.0160,  0.0726, -0.0541, -0.3328, -0.4206, -0.2578],\n",
       "        [ 0.0306,  0.0410,  0.0628,  0.2390,  0.4138,  0.3936,  0.1661],\n",
       "        [-0.0137, -0.0037, -0.0241, -0.0659, -0.1507, -0.0822, -0.0058]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs18pt.conv1.weight[0][0]\n",
    "# grad_fn=<SelectBackward> 属性消失，意味着这些参数不能参与反向传播等训练流程了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同时 requires_grad 属性变为 False\n",
    "rs18pt.conv1.weight[0].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用新层覆盖原来的层\n",
    "rs18pt.conv1 = nn.Conv2d(1,64,kernel_size=(7,7),stride=(2,2),padding=(3,3),bias=False)\n",
    "\n",
    "# 新生成的层默认 requires_grad=True\n",
    "# 因此在锁定模型中的参数后，只要覆盖掉原来的层，或者在原来的层之后加上新层，新层默认就是可以训练的\n",
    "# 但是新的层会覆盖掉原来层已经训练好的参数，所以一般不对conv1进行覆盖\n",
    "rs18pt.conv1.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照该逻辑定义架构\n",
    "# 让18层残差网络的前两个 layers 都被冻结，后面两个 layers 从0开始训练\n",
    "resnet18_ = m.resnet18() # 没有预训练的模型\n",
    "rs18pt = m.resnet18(pretrained=True) # resnet18_pretrained\n",
    "\n",
    "for param in rs18pt.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "fcin = rs18pt.fc.in_features\n",
    "\n",
    "class MyNet_pretrained(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 迁移层\n",
    "        self.pretrained = nn.Sequential(rs18pt.conv1,\n",
    "                                        rs18pt.bn1,\n",
    "                                        rs18pt.relu,\n",
    "                                        rs18pt.maxpool,\n",
    "                                        rs18pt.layer1,\n",
    "                                        rs18pt.layer2\n",
    "                                       )\n",
    "        # 允许训练的层\n",
    "        self.train_ = nn.Sequential(resnet18_.layer3\n",
    "                                   ,resnet18_.layer4\n",
    "                                   ,resnet18_.avgpool)\n",
    "        # 输出的线性层自己写，以确保输出的类别数量正确\n",
    "        self.fc = nn.Linear(in_features=fcin,out_features=10,bias=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.pretrained(x)\n",
    "        x = self.train_(x)\n",
    "        x = x.view(x.shape[0],512)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MyNet_pretrained()\n",
    "\n",
    "net.pretrained[0].weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train_[0][0].conv1.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MyNet_pretrained                              --                        --\n",
       "├─Sequential: 1-1                             [10, 128, 28, 28]         --\n",
       "│    └─Conv2d: 2-1                            [10, 64, 112, 112]        (9,408)\n",
       "│    └─BatchNorm2d: 2-2                       [10, 64, 112, 112]        (128)\n",
       "│    └─ReLU: 2-3                              [10, 64, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-4                         [10, 64, 56, 56]          --\n",
       "│    └─Sequential: 2-5                        [10, 64, 56, 56]          --\n",
       "│    │    └─BasicBlock: 3-1                   [10, 64, 56, 56]          (73,984)\n",
       "│    │    └─BasicBlock: 3-2                   [10, 64, 56, 56]          (73,984)\n",
       "│    └─Sequential: 2-6                        [10, 128, 28, 28]         --\n",
       "│    │    └─BasicBlock: 3-3                   [10, 128, 28, 28]         (230,144)\n",
       "│    │    └─BasicBlock: 3-4                   [10, 128, 28, 28]         (295,424)\n",
       "├─Sequential: 1-2                             [10, 512, 1, 1]           --\n",
       "│    └─Sequential: 2-7                        [10, 256, 14, 14]         --\n",
       "│    │    └─BasicBlock: 3-5                   [10, 256, 14, 14]         919,040\n",
       "│    │    └─BasicBlock: 3-6                   [10, 256, 14, 14]         1,180,672\n",
       "│    └─Sequential: 2-8                        [10, 512, 7, 7]           --\n",
       "│    │    └─BasicBlock: 3-7                   [10, 512, 7, 7]           3,673,088\n",
       "│    │    └─BasicBlock: 3-8                   [10, 512, 7, 7]           4,720,640\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [10, 512, 1, 1]           --\n",
       "├─Linear: 1-3                                 [10, 10]                  5,130\n",
       "===============================================================================================\n",
       "Total params: 11,181,642\n",
       "Trainable params: 10,498,570\n",
       "Non-trainable params: 683,072\n",
       "Total mult-adds (G): 18.14\n",
       "===============================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 397.39\n",
       "Params size (MB): 44.73\n",
       "Estimated Total Size (MB): 448.14\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(net,input_size=(10,3,224,224),depth=3,device=\"cpu\")\n",
    "# 输出中，不能训练的参数被括号括了起来，表示被锁定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一段时间后，希望解锁部分层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.pretrained # 提取前面锁住的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BasicBlock(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (1): BasicBlock(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.pretrained[5] # 第五个块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicBlock(\n",
       "  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.pretrained[5][1] # 第五个块中的第二个残差块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f89ea15a7b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.pretrained[5][1].parameters() # 该层中的所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解锁被锁定部分的最后一个layers\n",
    "for param in net.pretrained[5][1].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.pretrained[5][1].conv1.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MyNet_pretrained                              --                        --\n",
       "├─Sequential: 1-1                             [10, 128, 28, 28]         --\n",
       "│    └─Conv2d: 2-1                            [10, 64, 112, 112]        (9,408)\n",
       "│    └─BatchNorm2d: 2-2                       [10, 64, 112, 112]        (128)\n",
       "│    └─ReLU: 2-3                              [10, 64, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-4                         [10, 64, 56, 56]          --\n",
       "│    └─Sequential: 2-5                        [10, 64, 56, 56]          --\n",
       "│    │    └─BasicBlock: 3-1                   [10, 64, 56, 56]          (73,984)\n",
       "│    │    └─BasicBlock: 3-2                   [10, 64, 56, 56]          (73,984)\n",
       "│    └─Sequential: 2-6                        [10, 128, 28, 28]         --\n",
       "│    │    └─BasicBlock: 3-3                   [10, 128, 28, 28]         (230,144)\n",
       "│    │    └─BasicBlock: 3-4                   [10, 128, 28, 28]         295,424\n",
       "├─Sequential: 1-2                             [10, 512, 1, 1]           --\n",
       "│    └─Sequential: 2-7                        [10, 256, 14, 14]         --\n",
       "│    │    └─BasicBlock: 3-5                   [10, 256, 14, 14]         919,040\n",
       "│    │    └─BasicBlock: 3-6                   [10, 256, 14, 14]         1,180,672\n",
       "│    └─Sequential: 2-8                        [10, 512, 7, 7]           --\n",
       "│    │    └─BasicBlock: 3-7                   [10, 512, 7, 7]           3,673,088\n",
       "│    │    └─BasicBlock: 3-8                   [10, 512, 7, 7]           4,720,640\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [10, 512, 1, 1]           --\n",
       "├─Linear: 1-3                                 [10, 10]                  5,130\n",
       "===============================================================================================\n",
       "Total params: 11,181,642\n",
       "Trainable params: 10,793,994\n",
       "Non-trainable params: 387,648\n",
       "Total mult-adds (G): 18.14\n",
       "===============================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 397.39\n",
       "Params size (MB): 44.73\n",
       "Estimated Total Size (MB): 448.14\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(net,input_size=(10,3,224,224),depth=3,device=\"cpu\")\n",
    "# 输出中，不能训练的参数被括号括了起来，表示被锁定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 PyTorch 中我们可以非常灵活的调用任何层或层上的参数\n",
    "# 这为我们灵活调用层来组成自己希望的预训练模型提供较好的基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 来源：github / 实验室 获得的模型或权重\n",
    "# 格式：pt,pth\n",
    "# 否则需要先将模型文件转化为 pytorch 可以读取的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========【从url获取模型权重】=========\n",
    "url = 'http://xxx/xxx.pth'\n",
    "\n",
    "# 定义 model 架构，并实例化 model,model架构必须与 url 中权重要求的架构一模一样\n",
    "model = TheModelClass(*args,**kwargs)\n",
    "\n",
    "# state_dict 模型的完整形式，包括结构和参数\n",
    "state_dict = load_state_dict_from_url(url)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of MyNet_pretrained(\n",
       "  (pretrained): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (train_): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========【从保存好的权重文件中获取模型权重】=========\n",
    "PATH = 'xxx/xxx.pth'\n",
    "\n",
    "# 实例化模型\n",
    "model = TheModelClass(*args,**kwargs)\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========【从保存好的模型中获取模型权重】=========\n",
    "PATH = 'xxx/xxx.pth'\n",
    "\n",
    "model = torch.load(PATH)\n",
    "\n",
    "# 获取权重\n",
    "model.state_dict()\n",
    "best_model_wts = copy.deepcopy(model.state_dict()) # 深拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择对 state_dict() 中的部分值进行迭代\n",
    "model.load_state_dict(best_model_wts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
