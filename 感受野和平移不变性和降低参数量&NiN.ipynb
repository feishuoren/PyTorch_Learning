{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "扩大感受野的方法：\n",
    "- 加深卷积神经网络的深度（每增加一个卷积层，感受野的宽和高就会线性增加 `卷积核的尺寸-1`）\n",
    "- 使用池化层或其他快速消减特征图尺寸的技术\n",
    "- 使用更加丰富的卷积操作，如膨胀卷积/空洞卷积dilated convolution、残差连接等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感受野尺寸的计算：\n",
    "\n",
    "$r_l = r_{l-1} + (k_l - 1) * \\prod_{i=0}^{l-1}s_i$\n",
    "\n",
    "感受野的大小只与卷积核的大小、各层的步长有关，与padding无关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch_receptive_field import receptive_field\n",
    "\n",
    "# git clone https://github.com/Fangyh09/pytorch-receptive-field.git\n",
    "# 将receptive-field文件夹放到 python 安装目录或anaconda 安装目录的 site-pacckages文件夹下\n",
    "# $ pip -V 查看目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,6,5) # rl1 = 1 + (5 - 1 ) * (1) = 5\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2,stride=2) # rl2 = 5 + (2-1) * (1*1) = 6 \n",
    "        self.conv2 = nn.Conv2d(6,16,5) # rl3 = 6 + (5-1) * (1*1*2) = 14\n",
    "        self.pool2 = nn.AvgPool2d(2) # rl4 = 14 + (2-1) * (1*1*2*1) = 16\n",
    "#         self.fc1 = nn.Linear(5*5*16,120) # weight(120,400)\n",
    "#         self.fc2 = nn.Linear(120,84)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        # 线性前，数据拉平\n",
    "#         x = x.view(-1,5*5*16) # -1:占位符，自动计算\n",
    "#         x = F.tanh(self.fc1(x)) \n",
    "#         output = F.softmax(self.fc2(x),dim=1) # (samples,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = LeNet5().cuda()\n",
    "net = LeNet5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "        Layer (type)    map size      start       jump receptive_field \n",
      "==============================================================================\n",
      "        0               [32, 32]        0.5        1.0             1.0 \n",
      "        1               [28, 28]        2.5        1.0             5.0 \n",
      "        2               [14, 14]        3.0        2.0             6.0 \n",
      "        3               [10, 10]        7.0        2.0            14.0 \n",
      "        4                 [5, 5]        8.0        4.0            16.0 \n",
      "==============================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0',\n",
       "              OrderedDict([('j', 1.0),\n",
       "                           ('r', 1.0),\n",
       "                           ('start', 0.5),\n",
       "                           ('conv_stage', True),\n",
       "                           ('output_shape', [-1, 1, 32, 32])])),\n",
       "             ('1',\n",
       "              OrderedDict([('j', 1.0),\n",
       "                           ('r', 5.0),\n",
       "                           ('start', 2.5),\n",
       "                           ('input_shape', [-1, 1, 32, 32]),\n",
       "                           ('output_shape', [-1, 6, 28, 28])])),\n",
       "             ('2',\n",
       "              OrderedDict([('j', 2.0),\n",
       "                           ('r', 6.0),\n",
       "                           ('start', 3.0),\n",
       "                           ('input_shape', [-1, 6, 28, 28]),\n",
       "                           ('output_shape', [-1, 6, 14, 14])])),\n",
       "             ('3',\n",
       "              OrderedDict([('j', 2.0),\n",
       "                           ('r', 14.0),\n",
       "                           ('start', 7.0),\n",
       "                           ('input_shape', [-1, 6, 14, 14]),\n",
       "                           ('output_shape', [-1, 16, 10, 10])])),\n",
       "             ('4',\n",
       "              OrderedDict([('j', 4.0),\n",
       "                           ('r', 16.0),\n",
       "                           ('start', 8.0),\n",
       "                           ('input_shape', [-1, 16, 10, 10]),\n",
       "                           ('output_shape', [-1, 16, 5, 5])])),\n",
       "             ('input_size', (1, 32, 32))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receptive_field(net,(1,32,32)) # 输入的数据结构，这里的输入不包括样本书"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当你的PC上有GPU的时候，receptive_field函数会在gpu上自动运行。因此必须把输入函数的网络放到GPU上\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# net = Model().to(device)\n",
    "\n",
    "# 该包只能识别 Conv2d 和 MaxPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "        Layer (type)    map size      start       jump receptive_field \n",
      "==============================================================================\n",
      "        0               [32, 32]        0.5        1.0             1.0 \n",
      "        1               [28, 28]        2.5        1.0             5.0 \n",
      "        2               [14, 14]        3.0        2.0             6.0 \n",
      "        3               [10, 10]        7.0        2.0            14.0 \n",
      "        4                 [5, 5]        8.0        4.0            16.0 \n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "receptive_field_dict = receptive_field(net,(1,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加模型鲁棒性：\n",
    "- 平移不变形\n",
    "- 数据增强\n",
    "\n",
    "    对原有数据略微修改或合成来增加数据量\n",
    "    eg: 旋转、模糊、调高饱和度、放大、缩小、调高亮度、变形、镜面翻转、去纹理化、去颜色化（脱色）、边缘增强、显著边缘图（边缘检测）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "架构如何影响卷积神经网络的效果：\n",
    "1. 影响CNN效果的因子：最后一个卷积层后的特征图的数目，也被称作“最大感受野上的通道数”，通道数越大CNN，效果越好\n",
    "2. 池化层（争议），不能提供不变性甚至不能对卷积神经网络的效果有影响。池化层缩小特征图的功能可以由步长等于2的卷积层来代替\n",
    "\n",
    "    池化层不能提供完美的平移不变性，因此一定会存在信息损失和例外，但从放大感受野的角度来说，池化层应该对模型存在一定的影响。对池化层而言，最关键的是能够快速下采样，即快速减少特征图尺寸、减少模型所需的参数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "影响卷积神经网络模型参数量的两种方式：\n",
    "1. 这个层自带参数，其参数量与该层的超参数量的输入有关（全连接层、BN层）\n",
    "2. 这个层会影响feature map的尺寸，影响整体像素量和计算量，从而影响全连接层的输入（池化、padding、stride）\n",
    "\n",
    "卷积层两种都有\n",
    "\n",
    "dropout、激活函数等操作不影响参数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积层参数量计算：\n",
    "$N_{parameters} = (K_H * K_w *C_{in}) * C_{out} + C_{out}$\n",
    "\n",
    "- 卷积核参数量：$K_H * K_w$\n",
    "- 一次扫描输入$C_{in}$张特征图输出一张特征图: $(K_H * K_w) * C_{in} + 1$\n",
    "    - 权重:$(K_H * K_w) * C_{in}$\n",
    "    - 偏置: 1\n",
    "- $C_{out}$次扫描输出$C_{out}$张特征图：$（(K_H * K_w) * C_{in} + 1） *  C_{out}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "conv1 = nn.Conv2d(3,6,3) # 3*3*3*6 + 6 = 168\n",
    "conv2 = nn.Conv2d(6,4,3) # 3*3*6*4 + 4 = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.numel(),conv1.bias.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.weight.numel(),conv2.bias.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = nn.Conv2d(4,16,5,stride=2,padding=1) # 5*5*4*16 + 16 = 1616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3.weight.numel(),conv3.bias.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "减少参数量的方法：\n",
    "- 消减输入特征图数量\n",
    "- 消减输出特征图数量\n",
    "- 消减每个连接上的核的尺寸\n",
    "- 消减输入特征图与输出特征图之间的连接数量\n",
    "\n",
    "1. 瓶颈设计 两个1*1卷积核之间包裹其他卷积层 ResNet\n",
    "2. 分组卷积 groups 不影响偏置数量\n",
    "    - 不考虑偏置 $parameters = N_{group} * groups = (K_H * K_W * \\frac{C_{in}}{groups})* \\frac{C_{out}}{groups}  * groups = \\frac{1}{groups} (K_H*K_W*C_{in}*C_{out})$\n",
    "    - 考虑偏置 $parameters = N_{group} * groups = ((K_H * K_W * \\frac{C_{in}}{groups})* \\frac{C_{out}}{groups}  + \\frac{C_{out}}{groups})* groups \n",
    "    = \\frac{1}{groups} (K_H*K_W*C_{in}*C_{out}) + C_{out}$\n",
    "    - groups = C_in 的分组卷积叫“深度卷积” $parameters = K_H *K_W *C_{out} + C_{out}$\n",
    "3. 深度可分离卷积（分离卷积） 对一个深度卷积输出的一组特征图执行1*1卷积，在对特征图进行线性变换，两种卷积打包在一起成为一个block。（GoogLeNet）  \n",
    "不考虑偏置：\n",
    "$parameters = K_H *K_W *C_{out}^{depth} + C_{in}^{pair} * C_{out}^{pair}$\n",
    "\n",
    "假设1 * 1卷积层不改变特征图数量，则$C_{in}^{pair} = C_{out}^{pair} = C_{out}^{depth}$:\n",
    "\n",
    "$ratio = \\frac{parameters_{深度可分离卷积}}{parameters_{原始卷积}} = \\frac{1}{C_{in}^{depth}} + \\frac{C_{out}^{pair}}{K_H*K_W *C_{in}^{depth}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小卷积核，减少参数量\n",
    "- 两个3 * 3可以代替5 * 5卷积核\n",
    "- 三个3 * 3可以代替7 * 7卷积核\n",
    "- 两个1 * 1中间加3 * 3可以替代3 * 3\n",
    "\n",
    "1*1 卷积核 ，又叫“逐点卷积”，（MLP layer）\n",
    "作用：\n",
    "`用在卷积层之间，用于调整输出的通道数，协助大幅度降低计算量和参数量，从而协助加深网络深度，这一作用又被称为“跨通道信息交互”。`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(4,8,3) # 3*3*4*8 + 8 = 288 + 8 = 296\n",
    "conv1_group = nn.Conv2d(4,8,3,groups=2) # 1/2 * 288 + 8 = 152\n",
    "# 分组数最大等于Max（C_in,C_out）,并且要能被C_in,C_out）整除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 144)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.numel(),conv1_group.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(4,8,3,bias=False) # 3*3*4*8 + 8 = 288 + 8 = 296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度可分离卷积\n",
    "conv1_depthwise = nn.Conv2d(4,8,3,groups=4,bias=False) # 288/4 = 72\n",
    "conv1_pairwise = nn.Conv2d(8,8,1,bias=False) # 8 * 8 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4722222222222222"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratio = 1 / C_in_depth + C_out_pair / (K_H + K_W * C_in_depth)\n",
    "\n",
    "ratio = 1/4 + 8 / (3*3 * 4)\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4722222222222222"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(conv1_depthwise.weight.numel() + conv1_pairwise.weight.numel()) / conv1.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # block1\n",
    "        self.conv1 = nn.Conv2d(3,6,3)\n",
    "        self.conv2 = nn.Conv2d(6,4,3)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # block2\n",
    "        self.conv3 = nn.Conv2d(4,16,5,stride=2,padding=1)\n",
    "        self.conv4 = nn.Conv2d(16,3,5,stride=3,padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # FC\n",
    "        self.linear1 = nn.Linear(3*9*9,256)\n",
    "        self.linear2 = nn.Linear(256,256)\n",
    "        self.linear3 = nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
    "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))        \n",
    "        \n",
    "        x = x.view(-1,3*9*9)\n",
    "        \n",
    "        x = F.relu(self.linear1(F.dropout(x,p=0.5)))\n",
    "        x = F.relu(self.linear2(F.dropout(x,p=0.5)))\n",
    "        output = F.softmax(self.linear3(x),dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 4, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(4, 16, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "  (conv4): Conv2d(16, 3, kernel_size=(5, 5), stride=(3, 3), padding=(2, 2))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear1): Linear(in_features=243, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (linear3): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(size=(10,3,229,229))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0984, 0.0941, 0.1055, 0.1049, 0.0960, 0.1052, 0.0999, 0.0970, 0.1038,\n",
       "         0.0950],\n",
       "        [0.0983, 0.0939, 0.1054, 0.1049, 0.0971, 0.1054, 0.0993, 0.0975, 0.1036,\n",
       "         0.0947],\n",
       "        [0.0989, 0.0934, 0.1029, 0.1066, 0.0959, 0.1039, 0.1002, 0.0990, 0.1046,\n",
       "         0.0947],\n",
       "        [0.0984, 0.0938, 0.1042, 0.1061, 0.0963, 0.1046, 0.0996, 0.0993, 0.1047,\n",
       "         0.0930],\n",
       "        [0.0985, 0.0934, 0.1039, 0.1065, 0.0965, 0.1040, 0.1005, 0.0986, 0.1047,\n",
       "         0.0935],\n",
       "        [0.0991, 0.0932, 0.1026, 0.1060, 0.0973, 0.1043, 0.0996, 0.0991, 0.1045,\n",
       "         0.0943],\n",
       "        [0.0982, 0.0953, 0.1034, 0.1057, 0.0962, 0.1045, 0.0989, 0.0978, 0.1052,\n",
       "         0.0948],\n",
       "        [0.0979, 0.0938, 0.1036, 0.1054, 0.0962, 0.1050, 0.1002, 0.0983, 0.1059,\n",
       "         0.0936],\n",
       "        [0.0996, 0.0937, 0.1045, 0.1062, 0.0955, 0.1045, 0.1006, 0.0971, 0.1056,\n",
       "         0.0926],\n",
       "        [0.0974, 0.0942, 0.1031, 0.1045, 0.0968, 0.1046, 0.1019, 0.0992, 0.1054,\n",
       "         0.0928]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(3,6,3),\n",
    "                    nn.ReLU(inplace=True), # inplace=True 计算后替代原始的值\n",
    "                    nn.Conv2d(6,4,3),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.Conv2d(4,16,5,stride=2,padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(16,3,5,stride=3,padding=2),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.MaxPool2d(2),\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 9, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(data).shape # 卷积+池化操作之后得到的特征概念图尺寸大小以及特征图的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "        Layer (type)    map size      start       jump receptive_field \n",
      "==============================================================================\n",
      "        0             [229, 229]        0.5        1.0             1.0 \n",
      "        1             [227, 227]        1.5        1.0             3.0 \n",
      "        2             [227, 227]        1.5        1.0             3.0 \n",
      "        3             [225, 225]        2.5        1.0             5.0 \n",
      "        4             [225, 225]        2.5        1.0             5.0 \n",
      "        5             [112, 112]        3.0        2.0             6.0 \n",
      "        6               [55, 55]        5.0        4.0            14.0 \n",
      "        7               [55, 55]        5.0        4.0            14.0 \n",
      "        8               [19, 19]        5.0       12.0            30.0 \n",
      "        9               [19, 19]        5.0       12.0            30.0 \n",
      "        10                [9, 9]       11.0       24.0            42.0 \n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "from torch_receptive_field import receptive_field\n",
    "\n",
    "rfdict = receptive_field(net,(3,229,229))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features_ = nn.Sequential(nn.Conv2d(3,64,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.Conv2d(64,64,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.MaxPool2d(2)\n",
    "                                      ,nn.Conv2d(64,128,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.Conv2d(128,128,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.MaxPool2d(2)\n",
    "                                      ,nn.Conv2d(128,256,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.MaxPool2d(2)\n",
    "                                      ,nn.Conv2d(256,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.MaxPool2d(2)\n",
    "                                      ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                      ,nn.MaxPool2d(2)\n",
    "                                    )\n",
    "        self.clf_ = nn.Sequential(nn.Dropout(0.5)\n",
    "                                  ,nn.Linear(512*7*7,4096),nn.ReLU(inplace=True)\n",
    "                                  ,nn.Dropout(0.5)\n",
    "                                  ,nn.Linear(4096,4096),nn.ReLU(inplace=True)\n",
    "                                  ,nn.Linear(4096,1000),nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.features_(x) # 用特征提取的架构 提取特征\n",
    "        x = x.view(-1,512*7*7) # 调整数据结构，拉平数据\n",
    "        output = self.clf_(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(3,64,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.Conv2d(64,64,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.MaxPool2d(2)\n",
    "                      \n",
    "                      ,nn.Conv2d(64,128,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.Conv2d(128,128,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.MaxPool2d(2)\n",
    "                      \n",
    "                      ,nn.Conv2d(128,256,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.MaxPool2d(2)\n",
    "                      \n",
    "                      ,nn.Conv2d(256,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.MaxPool2d(2)\n",
    "                      \n",
    "                      ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)\n",
    "                      ,nn.MaxPool2d(2)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 7, 7])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.ones(size=(10,3,224,224))\n",
    "net(data).shape # 512个特征图，尺寸为7*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VGG16                                    --                        --\n",
       "├─Sequential: 1-1                        [10, 512, 7, 7]           --\n",
       "│    └─Conv2d: 2-1                       [10, 64, 224, 224]        1,792\n",
       "│    └─ReLU: 2-2                         [10, 64, 224, 224]        --\n",
       "│    └─Conv2d: 2-3                       [10, 64, 224, 224]        36,928\n",
       "│    └─ReLU: 2-4                         [10, 64, 224, 224]        --\n",
       "│    └─MaxPool2d: 2-5                    [10, 64, 112, 112]        --\n",
       "│    └─Conv2d: 2-6                       [10, 128, 112, 112]       73,856\n",
       "│    └─ReLU: 2-7                         [10, 128, 112, 112]       --\n",
       "│    └─Conv2d: 2-8                       [10, 128, 112, 112]       147,584\n",
       "│    └─ReLU: 2-9                         [10, 128, 112, 112]       --\n",
       "│    └─MaxPool2d: 2-10                   [10, 128, 56, 56]         --\n",
       "│    └─Conv2d: 2-11                      [10, 256, 56, 56]         295,168\n",
       "│    └─ReLU: 2-12                        [10, 256, 56, 56]         --\n",
       "│    └─Conv2d: 2-13                      [10, 256, 56, 56]         590,080\n",
       "│    └─ReLU: 2-14                        [10, 256, 56, 56]         --\n",
       "│    └─Conv2d: 2-15                      [10, 256, 56, 56]         590,080\n",
       "│    └─ReLU: 2-16                        [10, 256, 56, 56]         --\n",
       "│    └─MaxPool2d: 2-17                   [10, 256, 28, 28]         --\n",
       "│    └─Conv2d: 2-18                      [10, 512, 28, 28]         1,180,160\n",
       "│    └─ReLU: 2-19                        [10, 512, 28, 28]         --\n",
       "│    └─Conv2d: 2-20                      [10, 512, 28, 28]         2,359,808\n",
       "│    └─ReLU: 2-21                        [10, 512, 28, 28]         --\n",
       "│    └─Conv2d: 2-22                      [10, 512, 28, 28]         2,359,808\n",
       "│    └─ReLU: 2-23                        [10, 512, 28, 28]         --\n",
       "│    └─MaxPool2d: 2-24                   [10, 512, 14, 14]         --\n",
       "│    └─Conv2d: 2-25                      [10, 512, 14, 14]         2,359,808\n",
       "│    └─ReLU: 2-26                        [10, 512, 14, 14]         --\n",
       "│    └─Conv2d: 2-27                      [10, 512, 14, 14]         2,359,808\n",
       "│    └─ReLU: 2-28                        [10, 512, 14, 14]         --\n",
       "│    └─Conv2d: 2-29                      [10, 512, 14, 14]         2,359,808\n",
       "│    └─ReLU: 2-30                        [10, 512, 14, 14]         --\n",
       "│    └─MaxPool2d: 2-31                   [10, 512, 7, 7]           --\n",
       "├─Sequential: 1-2                        [10, 1000]                --\n",
       "│    └─Dropout: 2-32                     [10, 25088]               --\n",
       "│    └─Linear: 2-33                      [10, 4096]                102,764,544\n",
       "│    └─ReLU: 2-34                        [10, 4096]                --\n",
       "│    └─Dropout: 2-35                     [10, 4096]                --\n",
       "│    └─Linear: 2-36                      [10, 4096]                16,781,312\n",
       "│    └─ReLU: 2-37                        [10, 4096]                --\n",
       "│    └─Linear: 2-38                      [10, 1000]                4,097,000\n",
       "│    └─Softmax: 2-39                     [10, 1000]                --\n",
       "==========================================================================================\n",
       "Total params: 138,357,544\n",
       "Trainable params: 138,357,544\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 154.84\n",
       "==========================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 1084.54\n",
       "Params size (MB): 553.43\n",
       "Estimated Total Size (MB): 1643.99\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vgg,input_size=(10,3,224,224),device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN：物体检测（滑窗识别）\n",
    "\n",
    "使用1 * 1 卷积核代替全连接层（参数会变多）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全局平均池化，池化核尺寸等于特征图尺寸，得到 1 * 1 输出，可以替代全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(10,7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap = nn.AvgPool2d(7)\n",
    "\n",
    "gap(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(size=(10,3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Conv2d(3,192,5,padding=2),nn.ReLU(inplace=True)\n",
    "                                  ,nn.Conv2d(192,160,1),nn.ReLU(inplace=True)\n",
    "                                  ,nn.Conv2d(160,96,1),nn.ReLU(inplace=True)\n",
    "                                  ,nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "                                  ,nn.Dropout(0.25))\n",
    "        self.block2 = nn.Sequential(nn.Conv2d(96,192,5,padding=2),nn.ReLU(inplace=True)\n",
    "                                  ,nn.Conv2d(192,192,1),nn.ReLU(inplace=True)\n",
    "                                  ,nn.Conv2d(192,192,1),nn.ReLU(inplace=True)\n",
    "                                  ,nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "                                  ,nn.Dropout(0.25))\n",
    "        self.block3 = nn.Sequential(nn.Conv2d(192,192,3,padding=1),nn.ReLU(inplace=True)\n",
    "                                  ,nn.Conv2d(192,192,1),nn.ReLU(inplace=True)\n",
    "                                  ,nn.Conv2d(192,10,1),nn.ReLU(inplace=True)\n",
    "                                  ,nn.AvgPool2d(7,stride=1)\n",
    "                                  ,nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output = self.block3(self.block2(self.block1(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NiN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 1, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NiN                                      --                        --\n",
       "├─Sequential: 1-1                        [10, 96, 15, 15]          --\n",
       "│    └─Conv2d: 2-1                       [10, 192, 32, 32]         14,592\n",
       "│    └─ReLU: 2-2                         [10, 192, 32, 32]         --\n",
       "│    └─Conv2d: 2-3                       [10, 160, 32, 32]         30,880\n",
       "│    └─ReLU: 2-4                         [10, 160, 32, 32]         --\n",
       "│    └─Conv2d: 2-5                       [10, 96, 32, 32]          15,456\n",
       "│    └─ReLU: 2-6                         [10, 96, 32, 32]          --\n",
       "│    └─MaxPool2d: 2-7                    [10, 96, 15, 15]          --\n",
       "│    └─Dropout: 2-8                      [10, 96, 15, 15]          --\n",
       "├─Sequential: 1-2                        [10, 192, 7, 7]           --\n",
       "│    └─Conv2d: 2-9                       [10, 192, 15, 15]         460,992\n",
       "│    └─ReLU: 2-10                        [10, 192, 15, 15]         --\n",
       "│    └─Conv2d: 2-11                      [10, 192, 15, 15]         37,056\n",
       "│    └─ReLU: 2-12                        [10, 192, 15, 15]         --\n",
       "│    └─Conv2d: 2-13                      [10, 192, 15, 15]         37,056\n",
       "│    └─ReLU: 2-14                        [10, 192, 15, 15]         --\n",
       "│    └─MaxPool2d: 2-15                   [10, 192, 7, 7]           --\n",
       "│    └─Dropout: 2-16                     [10, 192, 7, 7]           --\n",
       "├─Sequential: 1-3                        [10, 10, 1, 1]            --\n",
       "│    └─Conv2d: 2-17                      [10, 192, 7, 7]           331,968\n",
       "│    └─ReLU: 2-18                        [10, 192, 7, 7]           --\n",
       "│    └─Conv2d: 2-19                      [10, 192, 7, 7]           37,056\n",
       "│    └─ReLU: 2-20                        [10, 192, 7, 7]           --\n",
       "│    └─Conv2d: 2-21                      [10, 10, 7, 7]            1,930\n",
       "│    └─ReLU: 2-22                        [10, 10, 7, 7]            --\n",
       "│    └─AvgPool2d: 2-23                   [10, 10, 1, 1]            --\n",
       "│    └─Softmax: 2-24                     [10, 10, 1, 1]            --\n",
       "==========================================================================================\n",
       "Total params: 966,986\n",
       "Trainable params: 966,986\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.12\n",
       "Forward/backward pass size (MB): 48.61\n",
       "Params size (MB): 3.87\n",
       "Estimated Total Size (MB): 52.60\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net,(10,3,32,32),device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
