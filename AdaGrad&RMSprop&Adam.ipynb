{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glorot条件：\n",
    "- Xavier初始化\n",
    "- BatchNormlization\n",
    "\n",
    "将学习率衰减加入优化算法：\n",
    "1. 线性衰减至固定的最小值\n",
    "2. 指数衰减\n",
    "3. 每次验证集误差提升时，按2～10之间的乘数对学习率进行衰减\n",
    "\n",
    "AdaGrad：使用每个维度权重梯度的大小来自适应调整学习率\n",
    "（Adaptive Subgradiwnt Methods 自适应次梯度方法）\n",
    "\n",
    "\n",
    "- gt所有样本梯度的平均数$g_t = \\frac{\\partial{L}}{\\partial{w}}=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_w{L(x^{(i)},y^{(i)},w)}$\n",
    "- Gt过往迭代中gt的平方和\n",
    "- 步长 $\\frac{\\eta}{\\sqrt{G_t+\\epsilon}}$ 梯度为分母，梯度大则步长小，梯度小则步长大\n",
    "- $w_t = w_{(t-1)}-\\frac{\\eta}{\\sqrt{diag(G_t)+\\epsilon*I}} \\odot g_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad 非常适合于稀疏矩阵（低频特征、占比少、目标范围小）。\n",
    "致命问题：迭代后期的Gt大、学习率非常小，容易出现梯度消失现象\n",
    "\n",
    "torch.optim.Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSprop\n",
    "均方根传播（Root Mean Square Propogation）\n",
    "继承AdaGrad几乎全部设置\n",
    "- $g_t = \\frac{\\partial{L}}{\\partial{w}}$\n",
    "- $G_t = \\alpha G_{t-1} + (1-\\alpha)g_t^2$ 小于AdaGrad的Gt\n",
    "- $\\Delta{t} = B\\Delta_{t-1} - \\frac{\\eta}{\\sqrt{G_t}+\\epsilon}* g_t$\n",
    "- $w_t = w_{t-1} + \\Delta{t}$\n",
    "\n",
    "动量法的迭代公式：\n",
    "- $V_t = \\gamma V_{t-1} - \\eta \\frac{L}{\\partial{w}}$\n",
    "- $w_t = w_{t-1} + v_t$\n",
    "\n",
    "torch.optim.RMSprop\n",
    "\n",
    "pytorch在RMSprop中有参数centered,对梯度gt中心化使得梯度迭代更加平稳，其流程可能如下：\n",
    "$$g_t = g_t-Var(g_t)$$\n",
    "或者(更可能)\n",
    "$$g_t = \\frac{g_t}{Var(g_t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在陌生架构上尝试优化算法时，RMSprop首选。\n",
    "- 缺点：让学习率变大（Gt可能小于$G_{t-1}$）的特性违反了优化算法中的“正定性”$0.95G_{t-1} + 0.05g_t^2 = G_{t-1} - 0.05G_{t-1} + 0.05g_t^2 = G_{t-1} + 0.05(g_t^2 - G_{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam 亚当法\n",
    "迭代流程：\n",
    "- $g_t = \\frac{\\partial{L}}{\\partial{w}}$\n",
    "- $V_t = \\beta_1 V_{t-1} + (1-\\beta_1)g_t$\n",
    "- $G_t = \\beta_2G_{t-1} + (1-\\beta_2)g_t^2$\n",
    "- $w_t = w_{t-1} - \\frac{\\eta}{\\sqrt{G_t}+\\epsilon} * V_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“偏差修正”(bias correction)\n",
    "- $\\hat{V_t} = \\frac{V_t}{1-\\beta_1} = \\frac{\\beta_1V_{t-1} + (1-\\beta_1)g_t}{1-\\beta_1} = \\frac{\\beta_1}{1-\\beta_1}V_{t-1}+g_t$\n",
    "- $\\hat{G_t} = \\frac{G_t}{1-\\beta_2} = \\frac{\\beta_2G_{t-1} + (1-\\beta_2)g_t^2}{1-\\beta_2} = \\frac{\\beta_2}{1-\\beta_2}G_{t-1}+g_t^2$\n",
    "\n",
    "$$w_t = w_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{G_t}+\\epsilon}} * \\hat{V_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.optim.Adam\n",
    "\n",
    "超参数 amsgrad: True为改进后的亚当，默认False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AMSGrad 迭代流程：\n",
    "- $g_t = \\frac{\\partial{L}}{\\partial{w}}$\n",
    "- $V_t = \\beta_1 V_{t-1} + (1-\\beta_1)g_t$\n",
    "- $G_t = \\beta_2G_{t-1} + (1-\\beta_2)g_t^2$\n",
    "- $\\hat{G_t} = max(\\hat{G_{t-1}},G_t)$ 每次使用历史最大Gt\n",
    "- $w_t = w_{t-1} - \\frac{\\eta}{\\sqrt{G_t}+\\epsilon} * V_{t}$\n",
    "\n",
    "实际使用中Adam和AMSGrad效果差不多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 权重衰减 weight_decay\n",
    "处理正则化、防止过拟合的方式\n",
    "\n",
    "在w进行迭代时，以某种方式直接施加在w上的一个衰减项，衰减的程度一般由超参数$\\lambda$控制\n",
    "\n",
    "以普通梯度下降为例，带有权重衰减的w的迭代公式为：\n",
    "$$w_t = (1-\\lambda)w_{t-1} - \\eta * g_t$$\n",
    "\n",
    "很多时候把对损失函数的L2正则化与权重衰减等价。但这个性质对任何除了`梯度下降`外的算法都不适用，如不适用于AdaGrad、RMSprop、Adam\n",
    "\n",
    "在Adam上施加的权重衰减表现如下：\n",
    "- $g_t=\\frac{\\partial{L}}{\\partial{w}} + \\lambda w$\n",
    "- $w_t = w_{t-1}-\\eta(\\frac{V-t}{\\sqrt{\\hat{G_t}+\\epsilon}}+\\lambda w_{t-1})$\n",
    "\n",
    "在设置权重衰减时，首先设置比较小的值，效果不好时，放弃这种正则化解决过拟合的方式选择其他方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化算法选择\n",
    "\n",
    "《A Comparison of Optimization Algorithms for Deep Learning》\n",
    "\n",
    "SGD - Nestrov 可以了解下"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
